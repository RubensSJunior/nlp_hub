{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99f90c4a-4071-4ece-8a7a-9a1c92f39c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import pandas as pd \n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63cc572-0460-4c10-8dd1-d4018df0ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "spc = spacy.load(\"pt_core_news_lg\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stopwor_inst = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc67f45-8e5b-440d-9ac5-c3d5f4e8d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conecta_mongo(usuario,senha):\n",
    "    client = MongoClient(\"mongodb://{}:{}@mongodb:27017/db_nlp\".format(usuario,senha))\n",
    "    return client\n",
    "\n",
    "def retorna_clean_twittes_text(client):\n",
    "    db = client[\"db_nlp\"]\n",
    "    collection = db[\"collection_clean_twittes_text\"]\n",
    "    list_tweets = []\n",
    "    for x in collection.find():\n",
    "            list_tweets.append(x)\n",
    "    return list_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eff67c4-3c45-47d0-ad89-84a5cbccbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cliente_mongo = conecta_mongo(\"user_twitter_script\",\"pass_twitter_script\")\n",
    "#list_tweets = retorna_clean_twittes_text(cliente_mongo)\n",
    "#df_my_tweets = pd.DataFrame(list_tweets)\n",
    "df_rw_pos_neg = pd.read_csv(\"../data/dataset_label_pos_neg.csv\")\n",
    "df_rw_pos_neg = df_rw_pos_neg.sample(40000,random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d48e5d-0877-4aae-b565-2bc4769b7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rw_pos_neg[\"sentiment_bin\"] = df_rw_pos_neg[\"sentiment\"].apply(lambda x: 0 if x == \"Negativo\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fa0bca-5d4b-46a8-a1e9-ad4d03f94a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpezaDeString(texto):\n",
    "    # Letras minúsculas\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\" +\", ' ', texto)\n",
    "    # \\n\n",
    "    texto = re.sub(r\"(\\\\n)+\", \"\",texto);\n",
    "    texto = re.sub(r\"(\\n)+\", \"\",texto);\n",
    "\n",
    "    # Use tokenize method\n",
    "    geek = tk.tokenize(texto)\n",
    "    sentence = []\n",
    "    for word in geek:\n",
    "        if  ('“' not in word)  & ('”' not in word) & ('…' not in word) & (\"#\" not in word) & (\"@\" not in word ) & (\"htt\" not in word ) & ( word[0] not in  string.punctuation) & (\".com\" not in word) & (\".br\" not in word) & (not word[0].isnumeric()):\n",
    "            if re.match(r\"[a-zA-Z]\", word):\n",
    "                sentence.append(unidecode(word))\n",
    "            else:\n",
    "                sentence.append((word))\n",
    "    sentenca = ' '.join(sentence)\n",
    "\n",
    "    palavras_sem_stopwords = [palavra for palavra in sentenca.split() if palavra not in stopwor_inst]\n",
    "    palavras_sem_stopwords = ' '.join(palavras_sem_stopwords)\n",
    "    # Espaços em branco\n",
    "    palavras_sem_ponto = re.sub(r\"\\.\", ' ', palavras_sem_stopwords)\n",
    "    palavras_sem_espaco_duplo = re.sub(r\" +\", ' ', palavras_sem_ponto)\n",
    "    palavras_sem_sem_k = re.sub(r\"kkk\", ':]', palavras_sem_espaco_duplo)\n",
    "    palavras_sem_sem_k = re.sub(r\":]k\", ':]', palavras_sem_sem_k)\n",
    "    palavras_sem_sem_k = re.sub(r\":]k\", ':]', palavras_sem_sem_k)\n",
    "    palavras_sem_sem_a = re.sub(r\"aaa\", 'a', palavras_sem_sem_k)\n",
    "    palavras_sem_sem_a = re.sub(r\"aaa\", 'a', palavras_sem_sem_a)\n",
    "    palavras_sem_sem_r = re.sub(r\"rrr\", 'r', palavras_sem_sem_a)\n",
    "    palavras_sem_sem_r = re.sub(r\"rrr\", 'r', palavras_sem_sem_r)\n",
    "    palavras_sem_sem_i = re.sub(r\"ii\", 'i', palavras_sem_sem_r)\n",
    "    palavras_sem_sem_i = re.sub(r\"ii\", 'i', palavras_sem_sem_i)\n",
    "    palavras_sem_sem_h = re.sub(r\"hh\", 'h', palavras_sem_sem_i)\n",
    "    palavras_sem_sem_h = re.sub(r\"hh\", 'h', palavras_sem_sem_h)\n",
    "    palavras_sem_sem_o = re.sub(r\"ooo\", 'o', palavras_sem_sem_h)\n",
    "    palavras_sem_sem_o = re.sub(r\"oo\", 'o', palavras_sem_sem_o)\n",
    "    palavras_sem_sem_e = re.sub(r\"eee\", 'e', palavras_sem_sem_o)\n",
    "    palavras_sem_sem_e = re.sub(r\"ee\", 'e', palavras_sem_sem_e)\n",
    "    \n",
    "    return palavras_sem_sem_e\n",
    "\n",
    "def transform_lemma(sentence):\n",
    "    lista_lemma = []\n",
    "\n",
    "    for palavra in spc(sentence):\n",
    "        if palavra.pos_ in [\"ADJ\",\"NOUN\",\"VERB\",\"ADV\",\"DET\",\"PROPN\"]:\n",
    "            lista_lemma.append(palavra.lemma_)\n",
    "    return ' '.join(lista_lemma)\n",
    "\n",
    "def transform_stemm(sentence):\n",
    "    lista_stemm = []\n",
    "    for palavra in spc(sentence):\n",
    "        \n",
    "        lista_stemm.append(stemmer.stem(palavra.text))\n",
    "    return ' '.join(lista_stemm)\n",
    "\n",
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0e578c-610a-4652-9f08-977c810dd4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rw_pos_neg[\"tweet_text_clean\"] = df_rw_pos_neg[\"tweet_text\"].apply(limpezaDeString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed98e15-667a-4a40-8bbd-af4a8f937032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rw_pos_neg[\"tweet_text_lemma\"] = df_rw_pos_neg[\"tweet_text_clean\"].apply(transform_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d0d95d-2a42-4805-bfa6-677eb9b537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bow_article  = CountVectorizer(tokenizer=tk.tokenize).fit(df_rw_pos_neg[\"tweet_text_lemma\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56bf6e6-0baf-4e6d-904d-570d02d9f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = bow_article.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a571754-2df8-448f-9452-66c968e8a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tk.tokenize)\n",
    "X_padding = vectorizer.fit_transform(df_rw_pos_neg[\"tweet_text_lemma\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c196b097-4fbe-48a7-8039-5b0b621718dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "X_padding_tf_idf = transformer.fit_transform(X_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6851e22d-d6f1-4f77-9cc5-cba8505215ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padding_tf_idf.toarray(), df_rw_pos_neg[\"sentiment_bin\"].values, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5dc314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ba2b7-b451-4d06-93f0-27f90d1b11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../data/model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(model, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2fde5c2-7213-414c-a85d-768dc1277558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c7c9d-2f8d-4fbc-af97-1902cd799ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913f289-0399-42d0-898e-46156f19db08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0eb7650236ad649532561a0bb09336eae6a470ad46c281d5d8431873b9e5bf00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
