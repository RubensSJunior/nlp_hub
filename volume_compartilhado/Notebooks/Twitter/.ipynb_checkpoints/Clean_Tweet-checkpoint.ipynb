{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71f99db2-2e1f-4d63-b280-42a949712e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "from pymongo import errors\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d784494e-7350-4c9e-807a-9634f9501c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.0 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(r\"data/model.pkl\", \"rb\") as input_file:\n",
    "    model = pickle.load( input_file)\n",
    "    \n",
    "with open(r\"data/bow_article.pkl\", \"rb\") as input_file:\n",
    "    bow_article = pickle.load( input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d864c63f-77ae-4185-8743-4d05fb0483a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "spc = spacy.load(\"pt_core_news_lg\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stopwor_inst = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4da44676-e1b4-4f71-a428-27c5df05f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conecta_mongo(usuario,senha):\n",
    "    client = MongoClient(\"mongodb://{}:{}@mongodb:27017/db_nlp\".format(usuario,senha))\n",
    "    return client\n",
    "\n",
    "def retorna_termos_validos(client):\n",
    "    db = client[\"db_nlp\"]\n",
    "    collection = db[\"collection_twittes\"]\n",
    "    list_tweets = []\n",
    "    for x in collection.find():\n",
    "            list_tweets.append(x)\n",
    "    return list_tweets\n",
    "\n",
    "def clean_dados(list_tweets,client,model,bow_article):\n",
    "    db = client[\"db_nlp\"]\n",
    "    df_tweets = pd.DataFrame()\n",
    "    df_tweets_metions = pd.DataFrame()\n",
    "    df_tweets_hastag = pd.DataFrame()\n",
    "    for tweet in list_tweets:\n",
    "\n",
    "        # tweet = json.dumps(tweet)\n",
    "        #tweet = json.loads(tweet)\n",
    "        tw = {}\n",
    "\n",
    "        tw[\"id_tweet\"] = tweet[\"id\"]\n",
    "        tw[\"created_at\"] = datetime.strftime(datetime.strptime(tweet[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "        tw[\"text\"] = tweet[\"full_text\"]\n",
    "        tw[\"text_cleaned\"] =  transform_lemma(limpezaDeString(tweet[\"full_text\"]))\n",
    "        tw[\"size_text\"] = tweet[\"display_text_range\"][1]\n",
    "        tw[\"id_user\"] = tweet[\"user\"][\"id\"]\n",
    "        tw[\"id_termo\"] = tweet[\"id_termo\"]\n",
    "                               \n",
    "        X_padding = bow_article.transform([transform_lemma(limpezaDeString(tweet[\"full_text\"]))])\n",
    "        transformer = TfidfTransformer()\n",
    "        X_padding_tf_idf = transformer.fit_transform(X_padding)\n",
    "        tw[\"sentiment_predicted\"] = str(model.predict(X_padding_tf_idf)[0])\n",
    "        print(tw)\n",
    "        try:\n",
    "            filter = { 'id_tweet': tw[\"id_tweet\"], 'id_termo': tw[\"id_termo\"] }\n",
    "            new_values = { \"$set\": tw }\n",
    "            db.collection_clean_twittes_text.update_one(filter, new_values, upsert=True)\n",
    "        except Exception as e:\n",
    "            print(\"An exception occurred ::\", e)\n",
    "\n",
    "\n",
    "def salva_json_on_mongo(json_tw,client):    \n",
    "    db = client[\"db_nlp\"]\n",
    "    for tweet in json_tw:\n",
    "        try:\n",
    "            db.collection_clean_twittes.insert_one(tweet._json)\n",
    "        except:\n",
    "            print(\"erro\")\n",
    "            \n",
    "\n",
    "def clean_text(texto):\n",
    "    '''\n",
    "    Função para converter todas as letras para sua forma minúscula, selecionar apenas as letras,\n",
    "    remover stopwords e lematizar o texto. \n",
    "    '''\n",
    "\n",
    "    def remove_tweet_ruido(tweets_text):\n",
    "        clean_text = re.sub(r'RT+', '', tweets_text) \n",
    "        clean_text = re.sub(r'@\\S+', '', clean_text)  \n",
    "        clean_text = re.sub(r'http\\S+', '', clean_text) \n",
    "        clean_text = clean_text.replace(\"\\n\", \" \")\n",
    "        return clean_text\n",
    "\n",
    "    ### Transforme as letras para minúscula ###\n",
    "    minusculas = texto.lower()\n",
    "    texto = remove_tweet_ruido(minusculas)\n",
    "    ### Selecione apenas as letras do texto ##\n",
    "    letras = re.findall(r'\\b[A-zÀ-úü]+\\b', texto)\n",
    "    return letras\n",
    "\n",
    "\n",
    "def fecha_tudo(client):\n",
    "    client.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def limpezaDeString(texto):\n",
    "    # Letras minúsculas\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\" +\", ' ', texto)\n",
    "    # \\n\n",
    "    texto = re.sub(r\"(\\\\n)+\", \"\",texto);\n",
    "    texto = re.sub(r\"(\\n)+\", \"\",texto);\n",
    "\n",
    "    # Use tokenize method\n",
    "    geek = tk.tokenize(texto)\n",
    "    sentence = []\n",
    "    for word in geek:\n",
    "        if  ('“' not in word)  & ('”' not in word) & ('…' not in word) & (\"#\" not in word) & (\"@\" not in word ) & (\"htt\" not in word ) & ( word[0] not in  string.punctuation) & (\".com\" not in word) & (\".br\" not in word) & (not word[0].isnumeric()):\n",
    "            if re.match(r\"[a-zA-Z]\", word):\n",
    "                sentence.append(unidecode(word))\n",
    "            else:\n",
    "                sentence.append((word))\n",
    "    sentenca = ' '.join(sentence)\n",
    "\n",
    "    palavras_sem_stopwords = [palavra for palavra in sentenca.split() if palavra not in stopwor_inst]\n",
    "    palavras_sem_stopwords = ' '.join(palavras_sem_stopwords)\n",
    "    # Espaços em branco\n",
    "    palavras_sem_ponto = re.sub(r\"\\.\", ' ', palavras_sem_stopwords)\n",
    "    palavras_sem_espaco_duplo = re.sub(r\" +\", ' ', palavras_sem_ponto)\n",
    "    palavras_sem_sem_k = re.sub(r\"kkk\", ':]', palavras_sem_espaco_duplo)\n",
    "    palavras_sem_sem_k = re.sub(r\":]k\", ':]', palavras_sem_sem_k)\n",
    "    palavras_sem_sem_k = re.sub(r\":]k\", ':]', palavras_sem_sem_k)\n",
    "    palavras_sem_sem_a = re.sub(r\"aaa\", 'a', palavras_sem_sem_k)\n",
    "    palavras_sem_sem_a = re.sub(r\"aaa\", 'a', palavras_sem_sem_a)\n",
    "    palavras_sem_sem_r = re.sub(r\"rrr\", 'r', palavras_sem_sem_a)\n",
    "    palavras_sem_sem_r = re.sub(r\"rrr\", 'r', palavras_sem_sem_r)\n",
    "    palavras_sem_sem_i = re.sub(r\"ii\", 'i', palavras_sem_sem_r)\n",
    "    palavras_sem_sem_i = re.sub(r\"ii\", 'i', palavras_sem_sem_i)\n",
    "    palavras_sem_sem_h = re.sub(r\"hh\", 'h', palavras_sem_sem_i)\n",
    "    palavras_sem_sem_h = re.sub(r\"hh\", 'h', palavras_sem_sem_h)\n",
    "    palavras_sem_sem_o = re.sub(r\"ooo\", 'o', palavras_sem_sem_h)\n",
    "    palavras_sem_sem_o = re.sub(r\"oo\", 'o', palavras_sem_sem_o)\n",
    "    palavras_sem_sem_e = re.sub(r\"eee\", 'e', palavras_sem_sem_o)\n",
    "    palavras_sem_sem_e = re.sub(r\"ee\", 'e', palavras_sem_sem_e)\n",
    "    \n",
    "    return palavras_sem_sem_e\n",
    "\n",
    "def transform_lemma(sentence):\n",
    "    lista_lemma = []\n",
    "\n",
    "    for palavra in spc(sentence):\n",
    "        if palavra.pos_ in [\"ADJ\",\"NOUN\",\"VERB\",\"ADV\",\"DET\",\"PROPN\"]:\n",
    "            lista_lemma.append(palavra.lemma_)\n",
    "    return ' '.join(lista_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409322f-571a-4d3c-9533-d7d0b0fc433a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be4a29ae-4c8d-4447-baa8-dad8d487593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente_mongo = conecta_mongo(\"user_twitter_script\",\"pass_twitter_script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "975ebc5b-4df4-4d77-a329-4b4364936645",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_tweeter = retorna_termos_validos(cliente_mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f751416-2bcd-414f-aa8d-1ed36d442e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dados(lista_tweeter[:500],cliente_mongo,model,bow_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe4bca-f18e-4910-bfaa-588e0b151bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f7e4b85-a24b-4761-8abb-027cfc864fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbb838-4d4c-44ac-ba62-fc26dedc6084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fa997-d648-47e3-9452-dc9262ed70bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b3141-dda1-4752-a231-1ebae76ecc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a4f3c-2c18-423a-8b96-0e9675aab9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ad6d1-5bac-4ae4-a27b-2ac7d1e78959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e2bef1-a5c8-4ed6-b63c-2fef88031022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55adfc2-3907-433a-a656-29c93dbe61b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
